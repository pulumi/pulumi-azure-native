// *** WARNING: this file was generated by pulumi. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.AzureNative.MachineLearningServices.V20240401Preview.Outputs
{

    /// <summary>
    /// Fixed training parameters that won't be swept over during AutoML NLP training.
    /// </summary>
    [OutputType]
    public sealed class NlpFixedParametersResponse
    {
        /// <summary>
        /// Number of steps to accumulate gradients over before running a backward pass.
        /// </summary>
        public readonly int? GradientAccumulationSteps;
        /// <summary>
        /// The learning rate for the training procedure.
        /// </summary>
        public readonly double? LearningRate;
        /// <summary>
        /// The type of learning rate schedule to use during the training procedure.
        /// </summary>
        public readonly string? LearningRateScheduler;
        /// <summary>
        /// The name of the model to train.
        /// </summary>
        public readonly string? ModelName;
        /// <summary>
        /// Number of training epochs.
        /// </summary>
        public readonly int? NumberOfEpochs;
        /// <summary>
        /// The batch size for the training procedure.
        /// </summary>
        public readonly int? TrainingBatchSize;
        /// <summary>
        /// The batch size to be used during evaluation.
        /// </summary>
        public readonly int? ValidationBatchSize;
        /// <summary>
        /// The warmup ratio, used alongside LrSchedulerType.
        /// </summary>
        public readonly double? WarmupRatio;
        /// <summary>
        /// The weight decay for the training procedure.
        /// </summary>
        public readonly double? WeightDecay;

        [OutputConstructor]
        private NlpFixedParametersResponse(
            int? gradientAccumulationSteps,

            double? learningRate,

            string? learningRateScheduler,

            string? modelName,

            int? numberOfEpochs,

            int? trainingBatchSize,

            int? validationBatchSize,

            double? warmupRatio,

            double? weightDecay)
        {
            GradientAccumulationSteps = gradientAccumulationSteps;
            LearningRate = learningRate;
            LearningRateScheduler = learningRateScheduler;
            ModelName = modelName;
            NumberOfEpochs = numberOfEpochs;
            TrainingBatchSize = trainingBatchSize;
            ValidationBatchSize = validationBatchSize;
            WarmupRatio = warmupRatio;
            WeightDecay = weightDecay;
        }
    }
}

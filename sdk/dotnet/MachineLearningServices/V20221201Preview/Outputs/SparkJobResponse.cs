// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.AzureNative.MachineLearningServices.V20221201Preview.Outputs
{

    /// <summary>
    /// Spark job definition.
    /// </summary>
    [OutputType]
    public sealed class SparkJobResponse
    {
        /// <summary>
        /// Archive files used in the job.
        /// </summary>
        public readonly ImmutableArray<string> Archives;
        /// <summary>
        /// Arguments for the job.
        /// </summary>
        public readonly string? Args;
        /// <summary>
        /// [Required] ARM resource ID of the code asset.
        /// </summary>
        public readonly string CodeId;
        /// <summary>
        /// ARM resource ID of the component resource.
        /// </summary>
        public readonly string? ComponentId;
        /// <summary>
        /// ARM resource ID of the compute resource.
        /// </summary>
        public readonly string? ComputeId;
        /// <summary>
        /// Spark configured properties.
        /// </summary>
        public readonly ImmutableDictionary<string, string>? Conf;
        /// <summary>
        /// The asset description text.
        /// </summary>
        public readonly string? Description;
        /// <summary>
        /// Display name of job.
        /// </summary>
        public readonly string? DisplayName;
        /// <summary>
        /// [Required] The entry to execute on startup of the job.
        /// </summary>
        public readonly Union<Outputs.SparkJobPythonEntryResponse, Outputs.SparkJobScalaEntryResponse> Entry;
        /// <summary>
        /// The ARM resource ID of the Environment specification for the job.
        /// </summary>
        public readonly string? EnvironmentId;
        /// <summary>
        /// The name of the experiment the job belongs to. If not set, the job is placed in the "Default" experiment.
        /// </summary>
        public readonly string? ExperimentName;
        /// <summary>
        /// Files used in the job.
        /// </summary>
        public readonly ImmutableArray<string> Files;
        /// <summary>
        /// Identity configuration. If set, this should be one of AmlToken, ManagedIdentity, UserIdentity or null.
        /// Defaults to AmlToken if null.
        /// </summary>
        public readonly object? Identity;
        /// <summary>
        /// Mapping of input data bindings used in the job.
        /// </summary>
        public readonly ImmutableDictionary<string, object>? Inputs;
        /// <summary>
        /// Is the asset archived?
        /// </summary>
        public readonly bool? IsArchived;
        /// <summary>
        /// Jar files used in the job.
        /// </summary>
        public readonly ImmutableArray<string> Jars;
        /// <summary>
        /// Enum to determine the type of job.
        /// Expected value is 'Spark'.
        /// </summary>
        public readonly string JobType;
        /// <summary>
        /// Mapping of output data bindings used in the job.
        /// </summary>
        public readonly ImmutableDictionary<string, object>? Outputs;
        /// <summary>
        /// The asset property dictionary.
        /// </summary>
        public readonly ImmutableDictionary<string, string>? Properties;
        /// <summary>
        /// Python files used in the job.
        /// </summary>
        public readonly ImmutableArray<string> PyFiles;
        /// <summary>
        /// Compute Resource configuration for the job.
        /// </summary>
        public readonly Outputs.SparkResourceConfigurationResponse? Resources;
        /// <summary>
        /// List of JobEndpoints.
        /// For local jobs, a job endpoint will have an endpoint value of FileStreamObject.
        /// </summary>
        public readonly ImmutableDictionary<string, Outputs.JobServiceResponse>? Services;
        /// <summary>
        /// Status of the job.
        /// </summary>
        public readonly string Status;
        /// <summary>
        /// Tag dictionary. Tags can be added, removed, and updated.
        /// </summary>
        public readonly ImmutableDictionary<string, string>? Tags;

        [OutputConstructor]
        private SparkJobResponse(
            ImmutableArray<string> archives,

            string? args,

            string codeId,

            string? componentId,

            string? computeId,

            ImmutableDictionary<string, string>? conf,

            string? description,

            string? displayName,

            Union<Outputs.SparkJobPythonEntryResponse, Outputs.SparkJobScalaEntryResponse> entry,

            string? environmentId,

            string? experimentName,

            ImmutableArray<string> files,

            object? identity,

            ImmutableDictionary<string, object>? inputs,

            bool? isArchived,

            ImmutableArray<string> jars,

            string jobType,

            ImmutableDictionary<string, object>? outputs,

            ImmutableDictionary<string, string>? properties,

            ImmutableArray<string> pyFiles,

            Outputs.SparkResourceConfigurationResponse? resources,

            ImmutableDictionary<string, Outputs.JobServiceResponse>? services,

            string status,

            ImmutableDictionary<string, string>? tags)
        {
            Archives = archives;
            Args = args;
            CodeId = codeId;
            ComponentId = componentId;
            ComputeId = computeId;
            Conf = conf;
            Description = description;
            DisplayName = displayName;
            Entry = entry;
            EnvironmentId = environmentId;
            ExperimentName = experimentName;
            Files = files;
            Identity = identity;
            Inputs = inputs;
            IsArchived = isArchived;
            Jars = jars;
            JobType = jobType;
            Outputs = outputs;
            Properties = properties;
            PyFiles = pyFiles;
            Resources = resources;
            Services = services;
            Status = status;
            Tags = tags;
        }
    }
}

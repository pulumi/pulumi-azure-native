// *** WARNING: this file was generated by pulumi. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.AzureNative.DataFactory.Inputs
{

    /// <summary>
    /// Execute spark job activity.
    /// </summary>
    public sealed class SynapseSparkJobDefinitionActivityArgs : global::Pulumi.ResourceArgs
    {
        [Input("arguments")]
        private InputList<object>? _arguments;

        /// <summary>
        /// User specified arguments to SynapseSparkJobDefinitionActivity.
        /// </summary>
        public InputList<object> Arguments
        {
            get => _arguments ?? (_arguments = new InputList<object>());
            set => _arguments = value;
        }

        /// <summary>
        /// The fully-qualified identifier or the main class that is in the main definition file, which will override the 'className' of the spark job definition you provide. Type: string (or Expression with resultType string).
        /// </summary>
        [Input("className")]
        public Input<object>? ClassName { get; set; }

        /// <summary>
        /// Spark configuration properties, which will override the 'conf' of the spark job definition you provide.
        /// </summary>
        [Input("conf")]
        public Input<object>? Conf { get; set; }

        /// <summary>
        /// The type of the spark config.
        /// </summary>
        [Input("configurationType")]
        public InputUnion<string, Pulumi.AzureNative.DataFactory.ConfigurationType>? ConfigurationType { get; set; }

        [Input("dependsOn")]
        private InputList<Inputs.ActivityDependencyArgs>? _dependsOn;

        /// <summary>
        /// Activity depends on condition.
        /// </summary>
        public InputList<Inputs.ActivityDependencyArgs> DependsOn
        {
            get => _dependsOn ?? (_dependsOn = new InputList<Inputs.ActivityDependencyArgs>());
            set => _dependsOn = value;
        }

        /// <summary>
        /// Activity description.
        /// </summary>
        [Input("description")]
        public Input<string>? Description { get; set; }

        /// <summary>
        /// Number of core and memory to be used for driver allocated in the specified Spark pool for the job, which will be used for overriding 'driverCores' and 'driverMemory' of the spark job definition you provide. Type: string (or Expression with resultType string).
        /// </summary>
        [Input("driverSize")]
        public Input<object>? DriverSize { get; set; }

        /// <summary>
        /// Number of core and memory to be used for executors allocated in the specified Spark pool for the job, which will be used for overriding 'executorCores' and 'executorMemory' of the spark job definition you provide. Type: string (or Expression with resultType string).
        /// </summary>
        [Input("executorSize")]
        public Input<object>? ExecutorSize { get; set; }

        /// <summary>
        /// The main file used for the job, which will override the 'file' of the spark job definition you provide. Type: string (or Expression with resultType string).
        /// </summary>
        [Input("file")]
        public Input<object>? File { get; set; }

        [Input("files")]
        private InputList<object>? _files;

        /// <summary>
        /// (Deprecated. Please use pythonCodeReference and filesV2) Additional files used for reference in the main definition file, which will override the 'files' of the spark job definition you provide.
        /// </summary>
        public InputList<object> Files
        {
            get => _files ?? (_files = new InputList<object>());
            set => _files = value;
        }

        [Input("filesV2")]
        private InputList<object>? _filesV2;

        /// <summary>
        /// Additional files used for reference in the main definition file, which will override the 'jars' and 'files' of the spark job definition you provide.
        /// </summary>
        public InputList<object> FilesV2
        {
            get => _filesV2 ?? (_filesV2 = new InputList<object>());
            set => _filesV2 = value;
        }

        /// <summary>
        /// Linked service reference.
        /// </summary>
        [Input("linkedServiceName")]
        public Input<Inputs.LinkedServiceReferenceArgs>? LinkedServiceName { get; set; }

        /// <summary>
        /// Activity name.
        /// </summary>
        [Input("name", required: true)]
        public Input<string> Name { get; set; } = null!;

        /// <summary>
        /// Number of executors to launch for this job, which will override the 'numExecutors' of the spark job definition you provide. Type: integer (or Expression with resultType integer).
        /// </summary>
        [Input("numExecutors")]
        public Input<object>? NumExecutors { get; set; }

        /// <summary>
        /// Status result of the activity when the state is set to Inactive. This is an optional property and if not provided when the activity is inactive, the status will be Succeeded by default.
        /// </summary>
        [Input("onInactiveMarkAs")]
        public InputUnion<string, Pulumi.AzureNative.DataFactory.ActivityOnInactiveMarkAs>? OnInactiveMarkAs { get; set; }

        /// <summary>
        /// Activity policy.
        /// </summary>
        [Input("policy")]
        public Input<Inputs.ActivityPolicyArgs>? Policy { get; set; }

        [Input("pythonCodeReference")]
        private InputList<object>? _pythonCodeReference;

        /// <summary>
        /// Additional python code files used for reference in the main definition file, which will override the 'pyFiles' of the spark job definition you provide.
        /// </summary>
        public InputList<object> PythonCodeReference
        {
            get => _pythonCodeReference ?? (_pythonCodeReference = new InputList<object>());
            set => _pythonCodeReference = value;
        }

        /// <summary>
        /// Scanning subfolders from the root folder of the main definition file, these files will be added as reference files. The folders named 'jars', 'pyFiles', 'files' or 'archives' will be scanned, and the folders name are case sensitive. Type: boolean (or Expression with resultType boolean).
        /// </summary>
        [Input("scanFolder")]
        public Input<object>? ScanFolder { get; set; }

        [Input("sparkConfig")]
        private InputMap<object>? _sparkConfig;

        /// <summary>
        /// Spark configuration property.
        /// </summary>
        public InputMap<object> SparkConfig
        {
            get => _sparkConfig ?? (_sparkConfig = new InputMap<object>());
            set => _sparkConfig = value;
        }

        /// <summary>
        /// Synapse spark job reference.
        /// </summary>
        [Input("sparkJob", required: true)]
        public Input<Inputs.SynapseSparkJobReferenceArgs> SparkJob { get; set; } = null!;

        /// <summary>
        /// Activity state. This is an optional property and if not provided, the state will be Active by default.
        /// </summary>
        [Input("state")]
        public InputUnion<string, Pulumi.AzureNative.DataFactory.ActivityState>? State { get; set; }

        /// <summary>
        /// The name of the big data pool which will be used to execute the spark batch job, which will override the 'targetBigDataPool' of the spark job definition you provide.
        /// </summary>
        [Input("targetBigDataPool")]
        public Input<Inputs.BigDataPoolParametrizationReferenceArgs>? TargetBigDataPool { get; set; }

        /// <summary>
        /// The spark configuration of the spark job.
        /// </summary>
        [Input("targetSparkConfiguration")]
        public Input<Inputs.SparkConfigurationParametrizationReferenceArgs>? TargetSparkConfiguration { get; set; }

        /// <summary>
        /// Type of activity.
        /// Expected value is 'SparkJob'.
        /// </summary>
        [Input("type", required: true)]
        public Input<string> Type { get; set; } = null!;

        [Input("userProperties")]
        private InputList<Inputs.UserPropertyArgs>? _userProperties;

        /// <summary>
        /// Activity user properties.
        /// </summary>
        public InputList<Inputs.UserPropertyArgs> UserProperties
        {
            get => _userProperties ?? (_userProperties = new InputList<Inputs.UserPropertyArgs>());
            set => _userProperties = value;
        }

        public SynapseSparkJobDefinitionActivityArgs()
        {
        }
        public static new SynapseSparkJobDefinitionActivityArgs Empty => new SynapseSparkJobDefinitionActivityArgs();
    }
}

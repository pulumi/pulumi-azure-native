# coding=utf-8
# *** WARNING: this file was generated by the Pulumi SDK Generator. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import warnings
import pulumi
import pulumi.runtime
from typing import Union
from .. import _utilities, _tables


class WorkspaceExperimentJob(pulumi.CustomResource):
    name: pulumi.Output[str]
    """
    The name of the resource.
    """
    properties: pulumi.Output[dict]
    """
    The properties associated with the Job.
      * `caffe2_settings` (`dict`) - Caffe2 job settings.
        * `command_line_args` (`str`) - Command line arguments that need to be passed to the python script.
        * `python_interpreter_path` (`str`) - The path to the Python interpreter.
        * `python_script_file_path` (`str`) - The python script to execute.

      * `caffe_settings` (`dict`) - Caffe job settings.
        * `command_line_args` (`str`) - Command line arguments that need to be passed to the Caffe job.
        * `config_file_path` (`str`) - Path of the config file for the job. This property cannot be specified if pythonScriptFilePath is specified.
        * `process_count` (`float`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
        * `python_interpreter_path` (`str`) - The path to the Python interpreter. The property can be specified only if the pythonScriptFilePath is specified.
        * `python_script_file_path` (`str`) - Python script to execute. This property cannot be specified if configFilePath is specified.

      * `chainer_settings` (`dict`) - Chainer job settings.
        * `command_line_args` (`str`) - Command line arguments that need to be passed to the python script.
        * `process_count` (`float`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
        * `python_interpreter_path` (`str`) - The path to the Python interpreter.
        * `python_script_file_path` (`str`) - The python script to execute.

      * `cluster` (`dict`) - Resource ID of the cluster associated with the job.
        * `id` (`str`) - The ID of the resource

      * `cntk_settings` (`dict`) - CNTK (aka Microsoft Cognitive Toolkit) job settings.
        * `command_line_args` (`str`) - Command line arguments that need to be passed to the python script or cntk executable.
        * `config_file_path` (`str`) - Specifies the path of the BrainScript config file. This property can be specified only if the languageType is 'BrainScript'.
        * `language_type` (`str`) - The language to use for launching CNTK (aka Microsoft Cognitive Toolkit) job. Valid values are 'BrainScript' or 'Python'.
        * `process_count` (`float`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
        * `python_interpreter_path` (`str`) - The path to the Python interpreter. This property can be specified only if the languageType is 'Python'.
        * `python_script_file_path` (`str`) - Python script to execute. This property can be specified only if the languageType is 'Python'.

      * `constraints` (`dict`) - Constraints associated with the Job.
        * `max_wall_clock_time` (`str`) - Max time the job can run. Default value: 1 week.

      * `container_settings` (`dict`) - If the container was downloaded as part of cluster setup then the same container image will be used. If not provided, the job will run on the VM.
        * `image_source_registry` (`dict`) - Information about docker image and docker registry to download the container from.
          * `credentials` (`dict`) - Credentials to access the private docker repository.
            * `password` (`str`) - User password to login to the docker repository. One of password or passwordSecretReference must be specified.
            * `password_secret_reference` (`dict`) - KeyVault Secret storing the password. Users can store their secrets in Azure KeyVault and pass it to the Batch AI service to integrate with KeyVault. One of password or passwordSecretReference must be specified.
              * `secret_url` (`str`) - The URL referencing a secret in the Key Vault.
              * `source_vault` (`dict`) - Fully qualified resource identifier of the Key Vault.

            * `username` (`str`) - User name to login to the repository.

          * `image` (`str`) - The name of the image in the image repository.
          * `server_url` (`str`) - URL for image repository.

        * `shm_size` (`str`) - Size of /dev/shm. Please refer to docker documentation for supported argument formats.

      * `creation_time` (`str`) - The creation time of the job.
      * `custom_mpi_settings` (`dict`) - Custom MPI job settings.
        * `command_line` (`str`) - The command line to be executed by mpi runtime on each compute node.
        * `process_count` (`float`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

      * `custom_toolkit_settings` (`dict`) - Custom tool kit job settings.
        * `command_line` (`str`) - The command line to execute on the master node.

      * `environment_variables` (`list`) - A collection of user defined environment variables to be setup for the job.
        * `name` (`str`) - The name of the environment variable.
        * `value` (`str`) - The value of the environment variable.

      * `execution_info` (`dict`) - Information about the execution of a job.
      * `execution_state` (`str`) - The current state of the job. Possible values are: queued - The job is queued and able to run. A job enters this state when it is created, or when it is awaiting a retry after a failed run. running - The job is running on a compute cluster. This includes job-level preparation such as downloading resource files or set up container specified on the job - it does not necessarily mean that the job command line has started executing. terminating - The job is terminated by the user, the terminate operation is in progress. succeeded - The job has completed running successfully and exited with exit code 0. failed - The job has finished unsuccessfully (failed with a non-zero exit code) and has exhausted its retry limit. A job is also marked as failed if an error occurred launching the job.
      * `execution_state_transition_time` (`str`) - The time at which the job entered its current execution state.
      * `horovod_settings` (`dict`) - Specifies the settings for Horovod job.
        * `command_line_args` (`str`) - Command line arguments that need to be passed to the python script.
        * `process_count` (`float`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
        * `python_interpreter_path` (`str`) - The path to the Python interpreter.
        * `python_script_file_path` (`str`) - The python script to execute.

      * `input_directories` (`list`) - A list of input directories for the job.
        * `id` (`str`) - The ID for the input directory. The job can use AZ_BATCHAI_INPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.
        * `path` (`str`) - The path to the input directory.

      * `job_output_directory_path_segment` (`str`) - A segment of job's output directories path created by Batch AI. Batch AI creates job's output directories under an unique path to avoid conflicts between jobs. This value contains a path segment generated by Batch AI to make the path unique and can be used to find the output directory on the node or mounted filesystem.
      * `job_preparation` (`dict`) - The specified actions will run on all the nodes that are part of the job
        * `command_line` (`str`) - The command line to execute. If containerSettings is specified on the job, this commandLine will be executed in the same container as job. Otherwise it will be executed on the node.

      * `mount_volumes` (`dict`) - Collection of mount volumes available to the job during execution. These volumes are mounted before the job execution and unmounted after the job completion. The volumes are mounted at location specified by $AZ_BATCHAI_JOB_MOUNT_ROOT environment variable.
        * `azure_blob_file_systems` (`list`) - A collection of Azure Blob Containers that are to be mounted to the cluster nodes.
          * `account_name` (`str`) - Name of the Azure storage account.
          * `container_name` (`str`) - Name of the Azure Blob Storage container to mount on the cluster.
          * `credentials` (`dict`) - Information about the Azure storage credentials.
            * `account_key` (`str`) - Storage account key. One of accountKey or accountKeySecretReference must be specified.
            * `account_key_secret_reference` (`dict`) - Information about KeyVault secret storing the storage account key. One of accountKey or accountKeySecretReference must be specified.

          * `mount_options` (`str`) - Mount options for mounting blobfuse file system.
          * `relative_mount_path` (`str`) - The relative path on the compute node where the Azure File container will be mounted. Note that all cluster level containers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level containers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

        * `azure_file_shares` (`list`) - A collection of Azure File Shares that are to be mounted to the cluster nodes.
          * `account_name` (`str`) - Name of the Azure storage account.
          * `azure_file_url` (`str`) - URL to access the Azure File.
          * `credentials` (`dict`) - Information about the Azure storage credentials.
          * `directory_mode` (`str`) - File mode for directories on the mounted file share. Default value: 0777.
          * `file_mode` (`str`) - File mode for files on the mounted file share. Default value: 0777.
          * `relative_mount_path` (`str`) - The relative path on the compute node where the Azure File share will be mounted. Note that all cluster level file shares will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file shares will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

        * `file_servers` (`list`) - A collection of Batch AI File Servers that are to be mounted to the cluster nodes.
          * `file_server` (`dict`) - Resource ID of the existing File Server to be mounted.
          * `mount_options` (`str`) - Mount options to be passed to mount command.
          * `relative_mount_path` (`str`) - The relative path on the compute node where the File Server will be mounted. Note that all cluster level file servers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file servers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
          * `source_directory` (`str`) - File Server directory that needs to be mounted. If this property is not specified, the entire File Server will be mounted.

        * `unmanaged_file_systems` (`list`) - A collection of unmanaged file systems that are to be mounted to the cluster nodes.
          * `mount_command` (`str`) - Mount command line. Note, Batch AI will append mount path to the command on its own.
          * `relative_mount_path` (`str`) - The relative path on the compute node where the unmanaged file system will be mounted. Note that all cluster level unmanaged file systems will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level unmanaged file systems will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

      * `node_count` (`float`) - The job will be gang scheduled on that many compute nodes
      * `output_directories` (`list`) - A list of output directories for the job.
        * `id` (`str`) - The ID of the output directory. The job can use AZ_BATCHAI_OUTPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.
        * `path_prefix` (`str`) - The prefix path where the output directory will be created. Note, this is an absolute path to prefix. E.g. $AZ_BATCHAI_MOUNT_ROOT/MyNFS/MyLogs. The full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.
        * `path_suffix` (`str`) - The suffix path where the output directory will be created. E.g. models. You can find the full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.

      * `provisioning_state` (`str`) - The provisioned state of the Batch AI job
      * `provisioning_state_transition_time` (`str`) - The time at which the job entered its current provisioning state.
      * `py_torch_settings` (`dict`) - pyTorch job settings.
        * `command_line_args` (`str`) - Command line arguments that need to be passed to the python script.
        * `communication_backend` (`str`) - Type of the communication backend for distributed jobs. Valid values are 'TCP', 'Gloo' or 'MPI'. Not required for non-distributed jobs.
        * `process_count` (`float`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
        * `python_interpreter_path` (`str`) - The path to the Python interpreter.
        * `python_script_file_path` (`str`) - The python script to execute.

      * `scheduling_priority` (`str`) - Scheduling priority associated with the job.
      * `secrets` (`list`) - A collection of user defined environment variables with secret values to be setup for the job. Server will never report values of these variables back.
        * `name` (`str`) - The name of the environment variable to store the secret value.
        * `value` (`str`) - The value of the environment variable. This value will never be reported back by Batch AI.
        * `value_secret_reference` (`dict`) - KeyVault store and secret which contains the value for the environment variable. One of value or valueSecretReference must be provided.

      * `std_out_err_path_prefix` (`str`) - The path where the Batch AI service stores stdout, stderror and execution log of the job.
      * `tensor_flow_settings` (`dict`) - TensorFlow job settings.
        * `master_command_line_args` (`str`) - Command line arguments that need to be passed to the python script for the master task.
        * `parameter_server_command_line_args` (`str`) - Command line arguments that need to be passed to the python script for the parameter server. Optional for single process jobs.
        * `parameter_server_count` (`float`) - The number of parameter server tasks. If specified, the value must be less than or equal to nodeCount. If not specified, the default value is equal to 1 for distributed TensorFlow training. This property can be specified only for distributed TensorFlow training.
        * `python_interpreter_path` (`str`) - The path to the Python interpreter.
        * `python_script_file_path` (`str`) - The python script to execute.
        * `worker_command_line_args` (`str`) - Command line arguments that need to be passed to the python script for the worker task. Optional for single process jobs.
        * `worker_count` (`float`) - The number of worker tasks. If specified, the value must be less than or equal to (nodeCount * numberOfGPUs per VM). If not specified, the default value is equal to nodeCount. This property can be specified only for distributed TensorFlow training.

      * `tool_type` (`str`) - Possible values are: cntk, tensorflow, caffe, caffe2, chainer, pytorch, custom, custommpi, horovod.
    """
    type: pulumi.Output[str]
    """
    The type of the resource.
    """
    def __init__(__self__, resource_name, opts=None, experiment_name=None, name=None, properties=None, resource_group_name=None, workspace_name=None, __props__=None, __name__=None, __opts__=None):
        """
        Information about a Job.

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[str] experiment_name: The name of the experiment. Experiment names can only contain a combination of alphanumeric characters along with dash (-) and underscore (_). The name must be from 1 through 64 characters long.
        :param pulumi.Input[str] name: The name of the job within the specified resource group. Job names can only contain a combination of alphanumeric characters along with dash (-) and underscore (_). The name must be from 1 through 64 characters long.
        :param pulumi.Input[dict] properties: The properties of the Job.
        :param pulumi.Input[str] resource_group_name: Name of the resource group to which the resource belongs.
        :param pulumi.Input[str] workspace_name: The name of the workspace. Workspace names can only contain a combination of alphanumeric characters along with dash (-) and underscore (_). The name must be from 1 through 64 characters long.

        The **properties** object supports the following:

          * `caffe2_settings` (`pulumi.Input[dict]`) - Settings for Caffe2 job.
            * `command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script.
            * `python_interpreter_path` (`pulumi.Input[str]`) - The path to the Python interpreter.
            * `python_script_file_path` (`pulumi.Input[str]`) - The python script to execute.

          * `caffe_settings` (`pulumi.Input[dict]`) - Settings for Caffe job.
            * `command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the Caffe job.
            * `config_file_path` (`pulumi.Input[str]`) - Path of the config file for the job. This property cannot be specified if pythonScriptFilePath is specified.
            * `process_count` (`pulumi.Input[float]`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
            * `python_interpreter_path` (`pulumi.Input[str]`) - The path to the Python interpreter. The property can be specified only if the pythonScriptFilePath is specified.
            * `python_script_file_path` (`pulumi.Input[str]`) - Python script to execute. This property cannot be specified if configFilePath is specified.

          * `chainer_settings` (`pulumi.Input[dict]`) - Settings for Chainer job.
            * `command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script.
            * `process_count` (`pulumi.Input[float]`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
            * `python_interpreter_path` (`pulumi.Input[str]`) - The path to the Python interpreter.
            * `python_script_file_path` (`pulumi.Input[str]`) - The python script to execute.

          * `cluster` (`pulumi.Input[dict]`) - Resource ID of the cluster on which this job will run.
            * `id` (`pulumi.Input[str]`) - The ID of the resource

          * `cntk_settings` (`pulumi.Input[dict]`) - Settings for CNTK (aka Microsoft Cognitive Toolkit) job.
            * `command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script or cntk executable.
            * `config_file_path` (`pulumi.Input[str]`) - Specifies the path of the BrainScript config file. This property can be specified only if the languageType is 'BrainScript'.
            * `language_type` (`pulumi.Input[str]`) - The language to use for launching CNTK (aka Microsoft Cognitive Toolkit) job. Valid values are 'BrainScript' or 'Python'.
            * `process_count` (`pulumi.Input[float]`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
            * `python_interpreter_path` (`pulumi.Input[str]`) - The path to the Python interpreter. This property can be specified only if the languageType is 'Python'.
            * `python_script_file_path` (`pulumi.Input[str]`) - Python script to execute. This property can be specified only if the languageType is 'Python'.

          * `constraints` (`pulumi.Input[dict]`) - Constraints associated with the Job.
            * `max_wall_clock_time` (`pulumi.Input[str]`) - Max time the job can run. Default value: 1 week.

          * `container_settings` (`pulumi.Input[dict]`) - Docker container settings for the job. If not provided, the job will run directly on the node.
            * `image_source_registry` (`pulumi.Input[dict]`) - Information about docker image and docker registry to download the container from.
              * `credentials` (`pulumi.Input[dict]`) - Credentials to access the private docker repository.
                * `password` (`pulumi.Input[str]`) - User password to login to the docker repository. One of password or passwordSecretReference must be specified.
                * `password_secret_reference` (`pulumi.Input[dict]`) - KeyVault Secret storing the password. Users can store their secrets in Azure KeyVault and pass it to the Batch AI service to integrate with KeyVault. One of password or passwordSecretReference must be specified.
                  * `secret_url` (`pulumi.Input[str]`) - The URL referencing a secret in the Key Vault.
                  * `source_vault` (`pulumi.Input[dict]`) - Fully qualified resource identifier of the Key Vault.

                * `username` (`pulumi.Input[str]`) - User name to login to the repository.

              * `image` (`pulumi.Input[str]`) - The name of the image in the image repository.
              * `server_url` (`pulumi.Input[str]`) - URL for image repository.

            * `shm_size` (`pulumi.Input[str]`) - Size of /dev/shm. Please refer to docker documentation for supported argument formats.

          * `custom_mpi_settings` (`pulumi.Input[dict]`) - Settings for custom MPI job.
            * `command_line` (`pulumi.Input[str]`) - The command line to be executed by mpi runtime on each compute node.
            * `process_count` (`pulumi.Input[float]`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

          * `custom_toolkit_settings` (`pulumi.Input[dict]`) - Settings for custom tool kit job.
            * `command_line` (`pulumi.Input[str]`) - The command line to execute on the master node.

          * `environment_variables` (`pulumi.Input[list]`) - A list of user defined environment variables which will be setup for the job.
            * `name` (`pulumi.Input[str]`) - The name of the environment variable.
            * `value` (`pulumi.Input[str]`) - The value of the environment variable.

          * `horovod_settings` (`pulumi.Input[dict]`) - Settings for Horovod job.
            * `command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script.
            * `process_count` (`pulumi.Input[float]`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
            * `python_interpreter_path` (`pulumi.Input[str]`) - The path to the Python interpreter.
            * `python_script_file_path` (`pulumi.Input[str]`) - The python script to execute.

          * `input_directories` (`pulumi.Input[list]`) - A list of input directories for the job.
            * `id` (`pulumi.Input[str]`) - The ID for the input directory. The job can use AZ_BATCHAI_INPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.
            * `path` (`pulumi.Input[str]`) - The path to the input directory.

          * `job_preparation` (`pulumi.Input[dict]`) - A command line to be executed on each node allocated for the job before tool kit is launched.
            * `command_line` (`pulumi.Input[str]`) - The command line to execute. If containerSettings is specified on the job, this commandLine will be executed in the same container as job. Otherwise it will be executed on the node.

          * `mount_volumes` (`pulumi.Input[dict]`) - Information on mount volumes to be used by the job. These volumes will be mounted before the job execution and will be unmounted after the job completion. The volumes will be mounted at location specified by $AZ_BATCHAI_JOB_MOUNT_ROOT environment variable.
            * `azure_blob_file_systems` (`pulumi.Input[list]`) - A collection of Azure Blob Containers that are to be mounted to the cluster nodes.
              * `account_name` (`pulumi.Input[str]`) - Name of the Azure storage account.
              * `container_name` (`pulumi.Input[str]`) - Name of the Azure Blob Storage container to mount on the cluster.
              * `credentials` (`pulumi.Input[dict]`) - Information about the Azure storage credentials.
                * `account_key` (`pulumi.Input[str]`) - Storage account key. One of accountKey or accountKeySecretReference must be specified.
                * `account_key_secret_reference` (`pulumi.Input[dict]`) - Information about KeyVault secret storing the storage account key. One of accountKey or accountKeySecretReference must be specified.

              * `mount_options` (`pulumi.Input[str]`) - Mount options for mounting blobfuse file system.
              * `relative_mount_path` (`pulumi.Input[str]`) - The relative path on the compute node where the Azure File container will be mounted. Note that all cluster level containers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level containers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

            * `azure_file_shares` (`pulumi.Input[list]`) - A collection of Azure File Shares that are to be mounted to the cluster nodes.
              * `account_name` (`pulumi.Input[str]`) - Name of the Azure storage account.
              * `azure_file_url` (`pulumi.Input[str]`) - URL to access the Azure File.
              * `credentials` (`pulumi.Input[dict]`) - Information about the Azure storage credentials.
              * `directory_mode` (`pulumi.Input[str]`) - File mode for directories on the mounted file share. Default value: 0777.
              * `file_mode` (`pulumi.Input[str]`) - File mode for files on the mounted file share. Default value: 0777.
              * `relative_mount_path` (`pulumi.Input[str]`) - The relative path on the compute node where the Azure File share will be mounted. Note that all cluster level file shares will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file shares will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

            * `file_servers` (`pulumi.Input[list]`) - A collection of Batch AI File Servers that are to be mounted to the cluster nodes.
              * `file_server` (`pulumi.Input[dict]`) - Resource ID of the existing File Server to be mounted.
              * `mount_options` (`pulumi.Input[str]`) - Mount options to be passed to mount command.
              * `relative_mount_path` (`pulumi.Input[str]`) - The relative path on the compute node where the File Server will be mounted. Note that all cluster level file servers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file servers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
              * `source_directory` (`pulumi.Input[str]`) - File Server directory that needs to be mounted. If this property is not specified, the entire File Server will be mounted.

            * `unmanaged_file_systems` (`pulumi.Input[list]`) - A collection of unmanaged file systems that are to be mounted to the cluster nodes.
              * `mount_command` (`pulumi.Input[str]`) - Mount command line. Note, Batch AI will append mount path to the command on its own.
              * `relative_mount_path` (`pulumi.Input[str]`) - The relative path on the compute node where the unmanaged file system will be mounted. Note that all cluster level unmanaged file systems will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level unmanaged file systems will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

          * `node_count` (`pulumi.Input[float]`) - Number of compute nodes to run the job on. The job will be gang scheduled on that many compute nodes.
          * `output_directories` (`pulumi.Input[list]`) - A list of output directories for the job.
            * `id` (`pulumi.Input[str]`) - The ID of the output directory. The job can use AZ_BATCHAI_OUTPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.
            * `path_prefix` (`pulumi.Input[str]`) - The prefix path where the output directory will be created. Note, this is an absolute path to prefix. E.g. $AZ_BATCHAI_MOUNT_ROOT/MyNFS/MyLogs. The full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.
            * `path_suffix` (`pulumi.Input[str]`) - The suffix path where the output directory will be created. E.g. models. You can find the full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.

          * `py_torch_settings` (`pulumi.Input[dict]`) - Settings for pyTorch job.
            * `command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script.
            * `communication_backend` (`pulumi.Input[str]`) - Type of the communication backend for distributed jobs. Valid values are 'TCP', 'Gloo' or 'MPI'. Not required for non-distributed jobs.
            * `process_count` (`pulumi.Input[float]`) - Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
            * `python_interpreter_path` (`pulumi.Input[str]`) - The path to the Python interpreter.
            * `python_script_file_path` (`pulumi.Input[str]`) - The python script to execute.

          * `scheduling_priority` (`pulumi.Input[str]`) - Scheduling priority associated with the job. Possible values: low, normal, high.
          * `secrets` (`pulumi.Input[list]`) - A list of user defined environment variables with secret values which will be setup for the job. Server will never report values of these variables back.
            * `name` (`pulumi.Input[str]`) - The name of the environment variable to store the secret value.
            * `value` (`pulumi.Input[str]`) - The value of the environment variable. This value will never be reported back by Batch AI.
            * `value_secret_reference` (`pulumi.Input[dict]`) - KeyVault store and secret which contains the value for the environment variable. One of value or valueSecretReference must be provided.

          * `std_out_err_path_prefix` (`pulumi.Input[str]`) - The path where the Batch AI service will store stdout, stderror and execution log of the job.
          * `tensor_flow_settings` (`pulumi.Input[dict]`) - Settings for Tensor Flow job.
            * `master_command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script for the master task.
            * `parameter_server_command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script for the parameter server. Optional for single process jobs.
            * `parameter_server_count` (`pulumi.Input[float]`) - The number of parameter server tasks. If specified, the value must be less than or equal to nodeCount. If not specified, the default value is equal to 1 for distributed TensorFlow training. This property can be specified only for distributed TensorFlow training.
            * `python_interpreter_path` (`pulumi.Input[str]`) - The path to the Python interpreter.
            * `python_script_file_path` (`pulumi.Input[str]`) - The python script to execute.
            * `worker_command_line_args` (`pulumi.Input[str]`) - Command line arguments that need to be passed to the python script for the worker task. Optional for single process jobs.
            * `worker_count` (`pulumi.Input[float]`) - The number of worker tasks. If specified, the value must be less than or equal to (nodeCount * numberOfGPUs per VM). If not specified, the default value is equal to nodeCount. This property can be specified only for distributed TensorFlow training.
        """
        if __name__ is not None:
            warnings.warn("explicit use of __name__ is deprecated", DeprecationWarning)
            resource_name = __name__
        if __opts__ is not None:
            warnings.warn("explicit use of __opts__ is deprecated, use 'opts' instead", DeprecationWarning)
            opts = __opts__
        if opts is None:
            opts = pulumi.ResourceOptions()
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.version is None:
            opts.version = _utilities.get_version()
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = dict()

            if experiment_name is None:
                raise TypeError("Missing required property 'experiment_name'")
            __props__['experiment_name'] = experiment_name
            if name is None:
                raise TypeError("Missing required property 'name'")
            __props__['name'] = name
            __props__['properties'] = properties
            if resource_group_name is None:
                raise TypeError("Missing required property 'resource_group_name'")
            __props__['resource_group_name'] = resource_group_name
            if workspace_name is None:
                raise TypeError("Missing required property 'workspace_name'")
            __props__['workspace_name'] = workspace_name
            __props__['type'] = None
        super(WorkspaceExperimentJob, __self__).__init__(
            'azurerm:batchai:WorkspaceExperimentJob',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name, id, opts=None):
        """
        Get an existing WorkspaceExperimentJob resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param str id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = dict()

        return WorkspaceExperimentJob(resource_name, opts=opts, __props__=__props__)

    def translate_output_property(self, prop):
        return _tables.CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop

    def translate_input_property(self, prop):
        return _tables.SNAKE_TO_CAMEL_CASE_TABLE.get(prop) or prop
